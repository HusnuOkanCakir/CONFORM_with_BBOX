{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"BraA-1fyqRNO"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wqestBNg6dJH"},"outputs":[],"source":["from google.colab import output\n","output.enable_custom_widget_manager()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nyOvZSoerkHR"},"outputs":[],"source":["import os\n","import sys\n","sys.path.append('.')\n","\n","from pathlib import Path\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YcmYcK89q9nx"},"outputs":[],"source":["conform_path = 'MyDrive/conform/CONFORM' #modify this depending on your folders\n","env_path = Path('/content/drive') / conform_path\n","\n","if str(env_path) not in sys.path:\n","    sys.path.append(str(env_path))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yZKLbGSC3lC7"},"outputs":[],"source":["# getting some warnings and errors here but the model is working somehow\n","# (ERROR: pip's dependency resolver does not currently take into account all the packages that are installed)\n","\n","# system packages\n","!apt-get update\n","!apt-get install -y libgmp-dev libgnutls30 libidn2-0 libjpeg-dev libtiff-dev libwebp-dev liblz4-dev libbz2-dev libarchive-dev libyaml-dev libzstd-dev\n","\n","# python packages\n","!pip install torch torchvision torchaudio accelerate==0.28.0 anyio==4.3.0 argon2-cffi==23.1.0 argon2-cffi-bindings==21.2.0 \\\n","            arrow==1.3.0 asttokens==2.4.1 async-lru==2.0.4 attrs==23.2.0 babel==2.14.0 beautifulsoup4==4.12.3 \\\n","            bleach==6.1.0 cffi==1.16.0 comm==0.2.2 debugpy==1.8.1 decorator==5.1.1 defusedxml==0.7.1 diffusers==0.21.4 \\\n","            executing==2.0.1 fastjsonschema==2.19.1 fqdn==1.5.1 fsspec==2024.3.1 h11==0.14.0 httpcore==1.0.5 httpx==0.27.0 \\\n","            huggingface-hub==0.22.2 importlib-metadata==7.1.0 ipykernel==6.29.4 ipython==8.23.0 ipywidgets==8.1.2 \\\n","            isoduration==20.11.0 jedi==0.19.1 joblib==1.3.2 json5==0.9.24 jsonpointer==2.4 jsonschema==4.21.1 \\\n","            jsonschema-specifications==2023.12.1 jupyter==1.0.0 jupyter-client==8.6.1 jupyter-console==6.6.3 \\\n","            jupyter-core==5.7.2 jupyter-events==0.10.0 jupyter-lsp==2.2.4 jupyter-server==2.13.0 jupyter-server-terminals==0.5.3 \\\n","            jupyterlab==4.1.5 jupyterlab-pygments==0.3.0 jupyterlab-server==2.25.4 jupyterlab-widgets==3.0.10 \\\n","            matplotlib-inline==0.1.6 mistune==3.0.2 nbclient==0.10.0 nbconvert==7.16.3 nbformat==5.10.4 nest-asyncio==1.6.0 \\\n","            notebook==7.1.2 notebook-shim==0.2.4 overrides==7.7.0 packaging==24.0 pandocfilters==1.5.1 parso==0.8.3 \\\n","            pexpect==4.9.0 platformdirs==4.2.0 prometheus-client==0.20.0 prompt-toolkit==3.0.43 psutil==5.9.8 ptyprocess==0.7.0 \\\n","            pure-eval==0.2.2 pycparser==2.22 pygments==2.17.2 python-dateutil==2.9.0.post0 python-json-logger==2.0.7 \\\n","            pytorch-metric-learning==2.5.0 pyzmq==25.1.2 qtconsole==5.5.1 qtpy==2.4.1 referencing==0.34.0 regex==2023.12.25 \\\n","            rfc3339-validator==0.1.4 rfc3986-validator==0.1.1 rpds-py==0.18.0 safetensors==0.4.2 scikit-learn==1.4.1.post1 \\\n","            scipy==1.13.0 send2trash==1.8.2 six==1.16.0 sniffio==1.3.1 soupsieve==2.5 stack-data==0.6.3 terminado==0.18.1 \\\n","            threadpoolctl==3.4.0 tinycss2==1.2.1 tokenizers==0.15.2 tornado==6.4 tqdm==4.66.2 traitlets==5.14.2 \\\n","            transformers==4.39.3 types-python-dateutil==2.9.0.20240316 uri-template==1.3.0 wcwidth==0.2.13 webcolors==1.13 \\\n","            webencodings==0.5.1 websocket-client==1.7.0 widgetsnbextension==4.0.10 zipp==3.18.1\n"]},{"cell_type":"markdown","metadata":{"id":"1nyk2lxvp_RC"},"source":["# Initialize Model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_DEaRBk6p_RH"},"outputs":[],"source":["import torch\n","from diffusers import StableDiffusionPipeline\n","from pipeline_conform import ConformPipeline\n","\n","pretrained_model_name_or_path = \"runwayml/stable-diffusion-v1-5\"\n","\n","sd_pipeline = StableDiffusionPipeline.from_pretrained(\n","    pretrained_model_name_or_path=pretrained_model_name_or_path,\n","    torch_dtype=torch.float16,\n",").to(\"cuda\")\n","\n","pipeline = ConformPipeline(\n","    vae=sd_pipeline.vae,\n","    text_encoder=sd_pipeline.text_encoder,\n","    tokenizer=sd_pipeline.tokenizer,\n","    unet=sd_pipeline.unet,\n","    scheduler=sd_pipeline.scheduler,\n","    safety_checker=sd_pipeline.safety_checker,\n","    feature_extractor=sd_pipeline.feature_extractor,\n",")"]},{"cell_type":"markdown","metadata":{"id":"OMUyyNFWp_RK"},"source":["# Hyperparameter"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ej3HW4D_p_RL"},"outputs":[],"source":["\n","num_inference_steps = 20 # Number of steps to run the model\n","guidance_scale = 7.5 # Guidance scale for diffusion\n","attn_res = (16, 16) # Resolution of the attention map to apply CONFORM\n","steps_to_save_attention_maps = list(range(num_inference_steps)) # Steps to save attention maps\n","max_iter_to_alter = 30 # Which steps to stop updating the latents\n","refinement_steps = 20 # Number of refinement steps\n","scale_factor = 20 # Scale factor for the optimization step\n","iterative_refinement_steps = [0, 1, 3, 5, 10, 20] # Iterative refinement steps\n","do_smoothing = True # Apply smoothing to the attention maps\n","smoothing_sigma = 0.5 # Sigma for the smoothing kernel\n","smoothing_kernel_size = 3 # Kernel size for the smoothing kernel\n","temperature = 0.5 # Temperature for the contrastive loss\n","softmax_normalize = False # Normalize the attention maps\n","softmax_normalize_attention_maps = False # Normalize the attention maps\n","add_previous_attention_maps = True # Add previous attention maps to the loss calculation\n","previous_attention_map_anchor_step = None # Use a specific step as the previous attention map\n","loss_fn = \"ntxent\" # Loss function to use\n","seed = 4913 # Seed for the generation\n","# seed = 4812 # Seed for the generation\n"]},{"cell_type":"markdown","metadata":{"id":"TdWrgZvGp_RN"},"source":["# Prompt"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jlpowu35p_RO"},"outputs":[],"source":["# prompt = \"cat under the table\"\n","#--------------------------------------------------------------------------------------------------------------------------------------------------------\n","prompt = \"car on the right of a bike\""]},{"cell_type":"markdown","metadata":{"id":"yk3AiyUYp_RP"},"source":["## Indices"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"57fXP4Vnp_RQ"},"outputs":[],"source":["ids = pipeline.tokenizer(prompt).input_ids\n","indices = {\n","    i: tok\n","    for tok, i in zip(pipeline.tokenizer.convert_ids_to_tokens(ids), range(len(ids)))\n","}\n","print(indices)"]},{"cell_type":"markdown","metadata":{"id":"kbKGpkebp_RR"},"source":["## Token Groups"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"N93I9BFsp_RR"},"outputs":[],"source":["# token_groups = [\n","#     [1],\n","#     [2, 4]\n","# ]\n","#--------------------------------------------------------------------------------------------------------------------------------------------------------\n","token_groups = [\n","    [1, 4],\n","    [7]\n","]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BJ9dYk79cGHx"},"outputs":[],"source":["# # Define bounding boxes for tokens [x_min, y_min, x_max, y_max] origin point is top left of the image\n","# # bounding_boxes = {\n","# #     1: [0.1, 0.5, 0.4, 0.9],  # Bounding box for \"cat\"\n","# #     4: [0.1, 0.1, 0.9, 0.4],  # Bounding box for \"table\"\n","# # }\n","\n","# bounding_boxes = {\n","#     1: [0.1, 0.1, 0.4, 0.9],  # Bounding box for \"cat\"\n","#     4: [0.1, 0.1, 0.4, 0.9],  # Bounding box for \"location\"\n","#     7: [0.4, 0.1, 0.9, 0.9],  # Bounding box for \"dog\"\n","# }"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1sgVYrBwkrJ9"},"outputs":[],"source":["# img_size = (16,16)\n","\n","# for token_idx, bbox in bounding_boxes.items():\n","#   for coord, size in zip(bbox, img_size*2):\n","#     print(int(coord * size))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4SNQGMwRZE2H"},"outputs":[],"source":["\n","# %cd /content/drive/MyDrive/DL/Diffusion-SpaceTime-Attn-main/attention_optimization/stable-diffusion\n","%cd /content/drive/MyDrive/conform/CONFORM/LayoutTransformer\n","\n","!pip install --upgrade pip==23.0.1\n","\n","# 1) (Optional) for torch 1.11.0 + CUDA 11.3\n","# !pip install torch==1.11.0+cu113 torchvision==0.12.0+cu113 -f https://download.pytorch.org/whl/cu113/torch_stable.html\n","\n","# 2) Install pinned versions\n","!pip install numpy==1.19.2\n","\n","# 3) Install environment_replicate.yml pip deps\n","!pip install albumentations==0.4.3 diffusers opencv-python==4.1.2.30 pudb==2019.2 \\\n","  invisible-watermark imageio==2.9.0 imageio-ffmpeg==0.4.2 \\\n","  pytorch-lightning==1.4.2 omegaconf==2.1.1 test-tube>=0.7.5 \\\n","  streamlit>=0.73.1 einops==0.3.0 torch-fidelity==0.3.0 \\\n","  transformers==4.19.2 torchmetrics==0.6.0 kornia==0.6\n","\n","# 4) Install Taming Transformers & CLIP from git-------------------------\n","# !pip install git+https://github.com/CompVis/taming-transformers.git@master\n","# !pip install git+https://github.com/openai/CLIP.git@main\n","\n","# 5) Additional packages\n","!pip install bounding-box==0.1.3 fairseq==0.12.2 spacy==3.5.1 nltk==3.8.1 inflect==6.0.2\n","!python -m spacy download en_core_web_sm\n","\n","# 6) Install local stable-diffusion package-------------------------\n","# !pip install -e .\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"b6blUuD5Y4aP"},"outputs":[],"source":["%cd /content/drive/MyDrive/conform/CONFORM/LayoutTransformer\n","\n","import nltk\n","nltk.download('wordnet')  # Download WordNet\n","nltk.download('stopwords')  # Download Stopwords\n","\n","# !python inference/inference_coco.py --sentence 'The silver bed was situated to the right of the white couch.'\n","\n","#--------------------------------------------------------------------------------------------------------------------------------------------------------\n","!python inference/inference_coco.py --sentence 'car is the right of a cup'\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"e5I5aWVZY1Um"},"outputs":[],"source":["# Sample input data\n","position1 = (0.386, 0.441)  # Center of 'cat' #----------------------------------------------------------------------------\n","position2 = (0.550, 0.620)  # Center of 'dog'\n","\n","# List of positions to be assigned to the token groups\n","positions_list = [position1, position2]\n","\n","# # Token groups\n","# token_groups = [\n","#     [1, 4],  # 'cat' and 'left' share the same box\n","#     [7]       # 'dog' has its own box\n","# ]\n","\n","# Coordinates offset to create bounding boxes\n","box_offset = 0.2\n","\n","# Function to calculate bounding box from center (x, y)\n","def create_bounding_box(center, offset=0.2):\n","    x_center, y_center = center\n","    xmin = max(x_center - offset, 0)\n","    ymin = max(y_center - offset, 0)\n","    xmax = min(x_center + offset, 1)\n","    ymax = min(y_center + offset, 1)\n","    return [xmin, ymin, xmax, ymax]\n","\n","# Create bounding boxes for each token group\n","bounding_boxes = {}\n","\n","# indices = {\n","#     0: '<|startoftext|>',\n","#     1: 'cat</w>',\n","#     2: 'on</w>',\n","#     3: 'the</w>',\n","#     4: 'left</w>',\n","#     5: 'of</w>',\n","#     6: 'a</w>',\n","#     7: 'dog</w>',\n","#     8: '<|endoftext|>',\n","# }\n","\n","# Positions for each word (normalized x, y)\n","positions = {}\n","\n","for idx, group in enumerate(token_groups):\n","    position = positions_list[idx]  # Get the corresponding position for the group\n","    for token_id in group:\n","        positions[token_id] = position\n","\n","print(\"Positions:\", positions)\n","\n","# Assign bounding boxes based on the positions and token groups\n","for group in token_groups:\n","    # Get the first token's position in the group\n","    first_token_id = group[0]\n","    position = positions[first_token_id]\n","    # Create bounding box\n","    bounding_box = create_bounding_box(position, box_offset)\n","    for token_id in group:\n","        bounding_boxes[token_id] = bounding_box\n","\n","print(\"Bounding boxes:\", bounding_boxes)\n","\n","\n","%cd /content/drive/MyDrive/conform/CONFORM/"]},{"cell_type":"markdown","metadata":{"id":"TFPTbAaSp_RS"},"source":["## CONFORM Output"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kkqj7Q8np_RS"},"outputs":[],"source":["images, attention_maps = pipeline(\n","    prompt=prompt,\n","    token_groups=token_groups,\n","    bounding_boxes=bounding_boxes,\n","    guidance_scale=guidance_scale,\n","    generator=torch.Generator(\"cuda\").manual_seed(seed),\n","    num_inference_steps=num_inference_steps,\n","    max_iter_to_alter=max_iter_to_alter,\n","    attn_res=attn_res,\n","    scale_factor=scale_factor,\n","    iterative_refinement_steps=iterative_refinement_steps,\n","    steps_to_save_attention_maps=steps_to_save_attention_maps,\n","    do_smoothing=do_smoothing,\n","    smoothing_sigma=smoothing_sigma,\n","    smoothing_kernel_size=smoothing_kernel_size,\n","    temperature=temperature,\n","    refinement_steps=refinement_steps,\n","    softmax_normalize=softmax_normalize,\n","    softmax_normalize_attention_maps=softmax_normalize_attention_maps,\n","    add_previous_attention_maps=add_previous_attention_maps,\n","    previous_attention_map_anchor_step=previous_attention_map_anchor_step,\n","    loss_fn=loss_fn,\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tUKhNrIfp_RT"},"outputs":[],"source":["images[0]"]},{"cell_type":"markdown","metadata":{"id":"2pjQZho7p_RU"},"source":["## SD Output"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DJp0PX4qp_RU"},"outputs":[],"source":["out = sd_pipeline(\n","    prompt=prompt,\n","    guidance_scale=guidance_scale,\n","    num_inference_steps=num_inference_steps,\n","    generator=torch.Generator(\"cuda\").manual_seed(seed),\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Vq2-F2Yjp_RU"},"outputs":[],"source":["out.images[0]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"02mkYQUvlP4o"},"outputs":[],"source":["attention_maps[0]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yL7aLWoAHCcN"},"outputs":[],"source":["attn_map = attention_maps[0][19]\n","\n","print(attn_map.shape)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OpJOWZhDIOYy"},"outputs":[],"source":["import matplotlib.pyplot as plt\n","\n","# Choose a token index\n","# token_idx = 76  # Change this to visualize other tokens\n","# attn_map_for_token = attn_map[:, :, token_idx]\n","\n","# plt.figure(figsize=(6, 6))\n","# plt.title(f\"Attention map for token {token_idx}\")\n","# plt.imshow(attn_map_for_token, cmap=\"plasma\")\n","# plt.colorbar()\n","# plt.show()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Swrwp1luKKfF"},"outputs":[],"source":["# Visualize the token corresponding to \"cat\"\n","# token_idx = 1\n","token_idx = 1 #----------------------------------------------------------------------------\n","attn_map_for_token = attn_map[:, :, token_idx]\n","\n","plt.figure(figsize=(6, 6))\n","plt.title(f\"Attention map for token index {token_idx} (first object)\")\n","plt.imshow(attn_map_for_token, cmap=\"plasma\")\n","plt.colorbar()\n","plt.show()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HCnOlpmt7dTH"},"outputs":[],"source":["# Visualize the token corresponding to \"cat\"\n","# token_idx = 1\n","token_idx = 4 #----------------------------------------------------------------------------\n","attn_map_for_token = attn_map[:, :, token_idx]\n","\n","plt.figure(figsize=(6, 6))\n","plt.title(f\"Attention map for token index {token_idx} (location)\")\n","plt.imshow(attn_map_for_token, cmap=\"plasma\")\n","plt.colorbar()\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lNjyEPRBKg6_"},"outputs":[],"source":["# Visualize the token corresponding to \"table\"\n","# token_idx = 4\n","token_idx = 7 #----------------------------------------------------------------------------\n","attn_map_for_token = attn_map[:, :, token_idx]\n","\n","plt.figure(figsize=(6, 6))\n","plt.title(f\"Attention map for token index {token_idx} (second object)\")\n","plt.imshow(attn_map_for_token, cmap=\"plasma\")\n","plt.colorbar()\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EEkcuPecKnVx"},"outputs":[],"source":["# Visualize the token corresponding to \"under\"\n","# token_idx = 2\n","# token_idx = 4\n","# attn_map_for_token = attn_map[:, :, token_idx]\n","\n","# plt.figure(figsize=(6, 6))\n","# plt.title(f\"Attention map for token index {token_idx}\")\n","# plt.imshow(attn_map_for_token, cmap=\"plasma\")\n","# plt.colorbar()\n","# plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AElMUEmirZhI"},"outputs":[],"source":["# Function to create bounding box mask\n","def create_bbox_mask(bbox, grid_size):\n","    mask = torch.ones(grid_size)\n","    x_min, y_min, x_max, y_max = [\n","        int(coord * size) for coord, size in zip(bbox, grid_size*2)\n","    ]\n","    mask[y_min:y_max, x_min:x_max] = 10\n","    return mask\n","\n","# Visualize the attention map with bounding box for \"cat\"\n","# token_idx = 1  # Token for \"cat\"\n","token_idx = 1 #----------------------------------------------------------------------------\n","attn_map_for_token = attn_map[:, :, token_idx].cpu().numpy()\n","\n","# Create bounding box mask\n","bbox_cat = bounding_boxes[token_idx]  # Get bounding box for \"cat\"\n","bbox_mask_cat = create_bbox_mask(bbox_cat, attn_map_for_token.shape)\n","\n","# attn_map[:, :, token_idx] *= bbox_mask_cat\n","\n","# Plot the attention map\n","plt.figure(figsize=(6, 6))\n","plt.title(f\"Attention map with bounding box for token {token_idx} \")\n","plt.imshow(attn_map_for_token, cmap=\"plasma\", alpha=0.8)\n","plt.colorbar(label=\"Attention\")\n","\n","# Overlay the bounding box mask\n","plt.imshow(bbox_mask_cat, cmap=\"gray\", alpha=0.3)  # Mask with transparency\n","plt.show()\n","\n","# Visualize the attention map with bounding box for \"table\"\n","# token_idx = 4  # Token for \"table\"\n","token_idx = 7 #----------------------------------------------------------------------------\n","attn_map_for_token = attn_map[:, :, token_idx].cpu().numpy()\n","\n","# Create bounding box mask\n","bbox_table = bounding_boxes[token_idx]  # Get bounding box for \"table\"\n","bbox_mask_table = create_bbox_mask(bbox_table, attn_map_for_token.shape)\n","\n","# Plot the attention map\n","plt.figure(figsize=(6, 6))\n","plt.title(f\"Attention map with bounding box for token {token_idx} \")\n","plt.imshow(attn_map_for_token, cmap=\"plasma\", alpha=0.8)\n","plt.colorbar(label=\"Attention\")\n","\n","# Overlay the bounding box mask\n","plt.imshow(bbox_mask_table, cmap=\"gray\", alpha=0.3)  # Mask with transparency\n","plt.show()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NPl9NPRBejNO"},"outputs":[],"source":["\n","# # %cd /content/drive/MyDrive/DL/Diffusion-SpaceTime-Attn-main/attention_optimization/stable-diffusion\n","# %cd /content/drive/MyDrive/conform/CONFORM/LayoutTransformer\n","\n","# !pip install --upgrade pip==23.0.1\n","\n","# # 1) (Optional) for torch 1.11.0 + CUDA 11.3\n","# !pip install torch==1.11.0+cu113 torchvision==0.12.0+cu113 -f https://download.pytorch.org/whl/cu113/torch_stable.html\n","\n","# # 2) Install pinned versions\n","# !pip install numpy==1.19.2\n","\n","# # 3) Install environment_replicate.yml pip deps\n","# !pip install albumentations==0.4.3 diffusers opencv-python==4.1.2.30 pudb==2019.2 \\\n","#   invisible-watermark imageio==2.9.0 imageio-ffmpeg==0.4.2 \\\n","#   pytorch-lightning==1.4.2 omegaconf==2.1.1 test-tube>=0.7.5 \\\n","#   streamlit>=0.73.1 einops==0.3.0 torch-fidelity==0.3.0 \\\n","#   transformers==4.19.2 torchmetrics==0.6.0 kornia==0.6\n","\n","# # 4) Install Taming Transformers & CLIP from git-------------------------\n","# # !pip install git+https://github.com/CompVis/taming-transformers.git@master\n","# # !pip install git+https://github.com/openai/CLIP.git@main\n","\n","# # 5) Additional packages\n","# !pip install bounding-box==0.1.3 fairseq==0.12.2 spacy==3.5.1 nltk==3.8.1 inflect==6.0.2\n","# !python -m spacy download en_core_web_sm\n","\n","# # 6) Install local stable-diffusion package-------------------------\n","# # !pip install -e .\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ax1ahPkb_SJk"},"outputs":[],"source":["# %cd /content/drive/MyDrive/conform/CONFORM/LayoutTransformer\n","\n","# import nltk\n","# nltk.download('wordnet')  # Download WordNet\n","# nltk.download('stopwords')  # Download Stopwords\n","\n","# # !python inference/inference_coco.py --sentence 'The silver bed was situated to the right of the white couch.'\n","\n","# !python inference/inference_coco.py --sentence 'cat on the left of a dog'\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sY-tD9fYB9X-"},"outputs":[],"source":["# # Sample input data\n","# position1 = (0.397, 0.432)  # Center of 'cat'\n","# position2 = (0.479, 0.503)  # Center of 'dog'\n","\n","# # List of positions to be assigned to the token groups\n","# positions_list = [position1, position2]\n","\n","# # # Token groups\n","# # token_groups = [\n","# #     [1, 4],  # 'cat' and 'left' share the same box\n","# #     [7]       # 'dog' has its own box\n","# # ]\n","\n","# # Coordinates offset to create bounding boxes\n","# box_offset = 0.2\n","\n","# # Function to calculate bounding box from center (x, y)\n","# def create_bounding_box(center, offset=0.2):\n","#     x_center, y_center = center\n","#     xmin = max(x_center - offset, 0)\n","#     ymin = max(y_center - offset, 0)\n","#     xmax = min(x_center + offset, 1)\n","#     ymax = min(y_center + offset, 1)\n","#     return [xmin, ymin, xmax, ymax]\n","\n","# # Create bounding boxes for each token group\n","# bounding_boxes = {}\n","\n","# # indices = {\n","# #     0: '<|startoftext|>',\n","# #     1: 'cat</w>',\n","# #     2: 'on</w>',\n","# #     3: 'the</w>',\n","# #     4: 'left</w>',\n","# #     5: 'of</w>',\n","# #     6: 'a</w>',\n","# #     7: 'dog</w>',\n","# #     8: '<|endoftext|>',\n","# # }\n","\n","# # Positions for each word (normalized x, y)\n","# positions = {}\n","\n","# for idx, group in enumerate(token_groups):\n","#     position = positions_list[idx]  # Get the corresponding position for the group\n","#     for token_id in group:\n","#         positions[token_id] = position\n","\n","# print(\"Positions:\", positions)\n","\n","# # Assign bounding boxes based on the positions and token groups\n","# for group in token_groups:\n","#     # Get the first token's position in the group\n","#     first_token_id = group[0]\n","#     position = positions[first_token_id]\n","#     # Create bounding box\n","#     bounding_box = create_bounding_box(position, box_offset)\n","#     for token_id in group:\n","#         bounding_boxes[token_id] = bounding_box\n","\n","# print(\"Bounding boxes:\", bounding_boxes)"]},{"cell_type":"code","source":["%cd /content/drive/MyDrive/conform/CONFORM/detrex\n","!git submodule init\n","!git submodule update\n","\n","!python -m pip install -e detectron2"],"metadata":{"id":"2vJyoENgeSgc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install -e ."],"metadata":{"id":"bNj7ELpriATW"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vEWM-oCOSx7Q"},"outputs":[],"source":["\n","%cd /content/drive/MyDrive/conform/CONFORM/detrex\n","\n","# download pretrained DAB-DETR model\n","!wget https://github.com/IDEA-Research/detrex-storage/releases/download/v0.1.0/dab_detr_r50_50ep.pth\n","\n","# download pretrained DINO model\n","!wget https://github.com/IDEA-Research/detrex-storage/releases/download/v0.2.1/dino_r50_4scale_12ep.pth\n","\n","# download the demo image\n","!wget https://github.com/IDEA-Research/detrex-storage/releases/download/v0.2.1/idea.jpg\n","\n","\n"]},{"cell_type":"code","source":["%cd /content/drive/MyDrive/conform/CONFORM/detrex/\n","!python demo/demo.py --config-file projects/dab_detr/configs/dab_detr_r50_50ep.py \\\n","                    --input \"./idea.jpg\" \\\n","                    --output \"./demo_output.jpg\" \\\n","                    --opts train.init_checkpoint=\"./dab_detr_r50_50ep.pth\"\n","\n","\n"],"metadata":{"id":"DCn9kYqzj6SR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!python demo/demo.py --config-file projects/dino/configs/dino_r50_4scale_12ep.py \\\n","                    --input \"./idea.jpg\" \\\n","                    --output \"./demo_output.jpg\" \\\n","                    --opts train.init_checkpoint=\"./dino_r50_4scale_12ep.pth\""],"metadata":{"id":"uz_M0Ar1IhtU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%cd /content/drive/MyDrive/conform/CONFORM/detrex/detectron2\n","\n","# !pip install -e .\n","import argparse\n","import glob\n","import multiprocessing as mp\n","import sys\n","import os\n","from tqdm import tqdm\n","\n","print(\"current working directory:\", os.getcwd(), \"\\n\")\n","print(\"\\n\".join(sys.path))\n","\n","# sys.path.insert(0, \"/content/drive/MyDrive/conform/CONFORM/detrex\")\n","from predictors import VisualizationDemo\n","from detectron2.checkpoint import DetectionCheckpointer\n","from detectron2.config import LazyConfig, instantiate\n","from detectron2.data.detection_utils import read_image\n","from detectron2.utils.logger import setup_logger\n","\n","import torch\n","\n","import pickle as pkl\n"],"metadata":{"id":"clTutysum7Ly"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"mNWv-3cnn_B7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"mEo2nJjX6wai"},"execution_count":null,"outputs":[]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.2"}},"nbformat":4,"nbformat_minor":0}